{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "id": "9YvNetPRPfzd"
   },
   "outputs": [],
   "source": [
    "#!pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {
    "id": "mzX0_DV9PLT5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "id": "BMPoZAOePeoz"
   },
   "outputs": [],
   "source": [
    "def get_data(name):\n",
    "    from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "    features, targets = np.array([]), np.array([])\n",
    "\n",
    "    # fetch dataset\n",
    "    if name == \"yeast\":\n",
    "        dataset = fetch_ucirepo(id=110)\n",
    "        features = dataset.data.features.to_numpy()\n",
    "        targets = dataset.data.targets.to_numpy()\n",
    "        targets = targets.ravel()\n",
    "    elif name == \"spambase\":\n",
    "        dataset = fetch_ucirepo(id=94)\n",
    "        features = dataset.data.features.to_numpy()\n",
    "        targets = dataset.data.targets.to_numpy()\n",
    "        targets = targets.ravel()\n",
    "    elif name == \"rice\":\n",
    "        dataset = fetch_ucirepo(id=545)\n",
    "        features = dataset.data.features.to_numpy()\n",
    "        targets = dataset.data.targets.to_numpy()\n",
    "        targets = targets.ravel()\n",
    "    elif name == \"churn\":\n",
    "        dataset = fetch_ucirepo(id=563)\n",
    "        features = dataset.data.features.to_numpy()\n",
    "        targets = dataset.data.targets.to_numpy()\n",
    "        targets = targets.ravel()\n",
    "    elif name == \"health_nutri\":\n",
    "        dataset = fetch_ucirepo(id=887)\n",
    "        features = dataset.data.features.to_numpy()\n",
    "        targets = dataset.data.targets.to_numpy()\n",
    "        targets = targets.ravel()\n",
    "    elif name == \"zoo\":    # Good small ds.\n",
    "        dataset = fetch_ucirepo(id=602)  # should be 111\n",
    "        features = dataset.data.features.to_numpy()\n",
    "        targets = dataset.data.targets.to_numpy()\n",
    "        targets = targets.ravel()\n",
    "    elif name == \"parkinsons\":    # 197x22 Good for all. Fit for comparisons\n",
    "        dataset = fetch_ucirepo(id=174)\n",
    "        features = dataset.data.features.to_numpy()\n",
    "        targets = dataset.data.targets.to_numpy()\n",
    "        targets = targets.ravel()\n",
    "    elif name == \"glass\":    # not that useful\n",
    "        dataset = fetch_ucirepo(id=42)\n",
    "        features = dataset.data.features.to_numpy()\n",
    "        targets = dataset.data.targets.to_numpy()\n",
    "        targets = targets.ravel()\n",
    "    elif name == \"scale\":  \n",
    "        dataset = fetch_ucirepo(id=12)\n",
    "        features = dataset.data.features.to_numpy()\n",
    "        targets = dataset.data.targets.to_numpy()\n",
    "        targets = targets.ravel()\n",
    "    elif name == \"wholesale\":  \n",
    "        dataset = fetch_ucirepo(id=257)\n",
    "        features = dataset.data.features.to_numpy()\n",
    "        targets = dataset.data.targets.to_numpy()\n",
    "        targets = targets.ravel()\n",
    "\n",
    "        # onehotencoder = OneHotEncoder(sparse_output=False)\n",
    "        # encoded = onehotencoder.fit_transform(features[:,0].reshape(-1, 1))\n",
    "        # features = np.concatenate((encoded, features[:, 1:]), axis=1)\n",
    "        \n",
    "    elif name == \"thyroid\":  #useless\n",
    "        data = pd.read_csv('ann-train.data', header=None)\n",
    "\n",
    "        # Assuming the last column is the class label\n",
    "        features = data.iloc[:, :-1]\n",
    "        labels = data.iloc[:, -1]\n",
    "        print(\"Features:\\n\", features.head())\n",
    "        print(\"Labels:\\n\", labels.head())\n",
    "    elif name == \"marketing\":   # awesome for ANN\n",
    "        data = pd.read_csv('marketing_campaign.csv', sep=\"\\t\")\n",
    "        # drop missing values\n",
    "        data = data.dropna()\n",
    "\n",
    "        features = data.iloc[:, :-1].to_numpy()\n",
    "        targets = data.iloc[:, -1].to_numpy()\n",
    "        targets = targets.ravel()\n",
    "\n",
    "        \n",
    "        onehotencoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "        encoded = onehotencoder.fit_transform(features[:,2].reshape(-1, 1))\n",
    "\n",
    "        ft1 = np.concatenate((features[:, :2], encoded, features[:, 3:]), axis=1)\n",
    "        # now we encode the marital status\n",
    "        encoded = onehotencoder.fit_transform(ft1[:,7].reshape(-1, 1))\n",
    "        ft2 = np.concatenate((ft1[:, :7], encoded, ft1[:, 8:]), axis=1)\n",
    "        #print(\"new features2 \\n\", ft2[0])\n",
    "\n",
    "        # removing the date column\n",
    "        ft2 = np.concatenate((ft2[:, :18], ft2[:, 19:]), axis=1)\n",
    "        features = ft2\n",
    "    elif name == \"diabetes-pima\":\n",
    "        data = pd.read_csv('diabetes.csv')\n",
    "        \n",
    "        features = data.iloc[:, :-1].to_numpy()\n",
    "        targets = data.iloc[:, -1].to_numpy()\n",
    "        targets = targets.ravel()\n",
    "\n",
    "\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {
    "id": "lkxw4gftTYLx"
   },
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    onehotencoder = OneHotEncoder(sparse_output=False)\n",
    "    encoded_categories = onehotencoder.fit_transform(labels.reshape(-1, 1))\n",
    "\n",
    "    return encoded_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "id": "l0_8dEYPPtwz"
   },
   "outputs": [],
   "source": [
    "def run_knn(data_name, encode_type=None):\n",
    "    X, y = get_data(data_name)\n",
    "    test_size = 0.20\n",
    "\n",
    "    if encode_type == \"onehot\":\n",
    "        y = encode_onehot(y)\n",
    "    elif encode_type == \"label\":\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "\n",
    "    # splitting into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=0)\n",
    "\n",
    "    # feature scaling\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "    # run grid_search for hyperparameter tuning.\n",
    "    # print(\"Running grid search\")\n",
    "    # best_params, best_scores = run_grid_search(X_train, y_train)\n",
    "    # print(f\"Grid search done. Best params: {best_params} \\nBest score: {best_scores}\")\n",
    "\n",
    "\n",
    "    #classifier = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
    "    classifier = KNeighborsClassifier(n_neighbors=3,algorithm=\"auto\",leaf_size=10)\n",
    "    # classifier = KNeighborsClassifier(n_neighbors=30, p=2, algorithm=\"auto\", leaf_size=10, weights=\"distance\") #{'algorithm': 'auto', 'leaf_size': 10, 'n_neighbors': 30, 'p': 1, 'weights': 'distance'}\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # predicting on training and test sets\n",
    "    y_pred_train = classifier.predict(X_train)\n",
    "    y_pred_test = classifier.predict(X_test)\n",
    "\n",
    "    print(\"pridiction done.\")\n",
    "    # cm_train = confusion_matrix(y_train, y_pred_train)\n",
    "    print(f\"KNN Train-Test for {data_name}= {accuracy_score(y_train, y_pred_train)}, {accuracy_score(y_test, y_pred_test)}\")\n",
    "    #print(f\"KNN Out-sample prediction for {data_name}= {accuracy_score(y_test, y_pred_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {
    "id": "h9S3EE-KSvOz"
   },
   "outputs": [],
   "source": [
    "def run_svm(data_name, encode_type=None):\n",
    "    X, y = get_data(data_name)\n",
    "    test_size = 0.20\n",
    "\n",
    "    if encode_type == \"onehot\":\n",
    "        y = encode_onehot(y)\n",
    "    elif encode_type == \"label\":\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "\n",
    "    # splitting into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=0)\n",
    "\n",
    "    # feature scaling\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "    # run grid_search for hyperparameter tuning.\n",
    "    # print(\"Running grid search\")\n",
    "    # best_params, best_scores = run_grid_search(X_train, y_train)\n",
    "    # print(f\"Grid search done. Best params: {best_params} \\nBest score: {best_scores}\")\n",
    "\n",
    "    # training on train set\n",
    "    classifier = SVC(kernel=\"poly\", degree=1, random_state=0)\n",
    "    #classifier = SVC(kernel=\"poly\", degree=4, random_state=0)\n",
    "    #classifier = SVC(kernel=\"rbf\", degree=3, random_state=0)\n",
    "   \n",
    "    classifier.fit(X_train, y_train)\n",
    "  \n",
    "    # predicting on training and test sets\n",
    "    y_pred_train = classifier.predict(X_train)\n",
    "    y_pred_test = classifier.predict(X_test)\n",
    "\n",
    "\n",
    "    #cm_train = confusion_matrix(y_train, y_pred_train)\n",
    "    print(f\"SVM TrainTest for {data_name}= {accuracy_score(y_train, y_pred_train)}, {accuracy_score(y_test, y_pred_test)}\")\n",
    "    #print(f\"SVM Out-sample prediction for {data_name}= {accuracy_score(y_test, y_pred_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {
    "id": "3t020MpUTu42"
   },
   "outputs": [],
   "source": [
    "def run_ann(data_name, encode_type=None):\n",
    "    import random\n",
    "    seed_value = 0\n",
    "    # 1. Set seed for Python's random module\n",
    "    random.seed(seed_value)\n",
    "    # 2. Set seed for NumPy\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "    # 3. Set seed for TensorFlow\n",
    "    tf.random.set_seed(seed_value)\n",
    "\n",
    "\n",
    "    X, y = get_data(data_name)\n",
    "    test_size = 0.25\n",
    "\n",
    "    if encode_type == \"onehot\":\n",
    "        y = encode_onehot(y)\n",
    "    elif encode_type == \"label\":\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "\n",
    "    # splitting into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=0)\n",
    "\n",
    "    # feature scaling\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "    # run grid_search for hyperparameter tuning.\n",
    "\n",
    "    # create the ANN model\n",
    "    # initialize the ann\n",
    "    ann = tf.keras.models.Sequential()\n",
    "    ann.add(tf.keras.layers.Dense(units=X.shape[1], activation='relu')) # input and first hidden layer\n",
    "    ann.add(tf.keras.layers.Dense(units=64, activation='relu'))  # 2nd hidden layer\n",
    "    ann.add(tf.keras.layers.Dense(units=64, activation='relu'))  # 3rd hidden layer\n",
    "    ann.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
    "\n",
    "    if y.ndim == 1:\n",
    "        ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "        ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    else:\n",
    "        ann.add(tf.keras.layers.Dense(units=y.shape[1], activation='softmax'))\n",
    "        ann.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    ann.fit(X_train, y_train, batch_size=32, epochs=90, shuffle=False)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_train = ann.predict(X_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss_train, accuracy_train = ann.evaluate(X_train, y_train)\n",
    "    loss_test, accuracy_test = ann.evaluate(X_test, y_test)\n",
    "    print(f\"ANN TrainTest Accuracy: {accuracy_train:.2f}, {accuracy_test:.2f}\")\n",
    "    #print(f\"ANN Test Accuracy: {accuracy_test:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uvGXSDWZRk_G",
    "outputId": "33360764-3ef9-4be6-ecdd-a9bea104c67c"
   },
   "outputs": [
    {
     "ename": "DatasetNotFoundError",
     "evalue": "\nInvalid `prisma.donated_datasets.findFirst()` invocation:\n\n\nTimed out fetching a new connection from the connection pool. More info: http://pris.ly/d/connection-pool (Current connection pool timeout: 10, connection limit: 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[604], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# run_ann(\"zoo\")   # perfect against ann\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#run_knn(\"marketing\")\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# run_ann(\"marketing\")  # best for ANN\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#run_svm(\"parkinsons\")\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#run_svm(\"glass\")    # don't encode y here\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#run_ann(\"glass\", encode_type=\"onehot\")\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mrun_knn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43monehot\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m run_svm(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrice\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m run_ann(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrice\u001b[39m\u001b[38;5;124m\"\u001b[39m, encode_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monehot\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[601], line 2\u001b[0m, in \u001b[0;36mrun_knn\u001b[1;34m(data_name, encode_type)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_knn\u001b[39m(data_name, encode_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m----> 2\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     test_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.20\u001b[39m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m encode_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monehot\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[1;32mIn[599], line 18\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     16\u001b[0m     targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mravel()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrice\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 18\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_ucirepo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m545\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     features \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[0;32m     20\u001b[0m     targets \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mtargets\u001b[38;5;241m.\u001b[39mto_numpy()\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\ucimlrepo\\fetch.py:76\u001b[0m, in \u001b[0;36mfetch_ucirepo\u001b[1;34m(name, id)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m     75\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset not found in repository\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(error_msg)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# extract ID, name, and URL from metadata\u001b[39;00m\n\u001b[0;32m     80\u001b[0m metadata \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mDatasetNotFoundError\u001b[0m: \nInvalid `prisma.donated_datasets.findFirst()` invocation:\n\n\nTimed out fetching a new connection from the connection pool. More info: http://pris.ly/d/connection-pool (Current connection pool timeout: 10, connection limit: 17)"
     ]
    }
   ],
   "source": [
    "# run_ann(\"zoo\")   # perfect against ann\n",
    "#run_knn(\"marketing\")\n",
    "# run_ann(\"marketing\")  # best for ANN\n",
    "#run_svm(\"parkinsons\")\n",
    "#run_svm(\"glass\")    # don't encode y here\n",
    "#run_ann(\"glass\", encode_type=\"onehot\")\n",
    "run_knn(\"rice\", encode_type=\"onehot\")\n",
    "run_svm(\"rice\")\n",
    "run_ann(\"rice\", encode_type=\"onehot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCVDKHhmdjc8"
   },
   "outputs": [],
   "source": [
    "# X, y = get_data(\"wholesale\")\n",
    "# print(y[0:5])\n",
    "# X[0]\n",
    "#data.isna().sum()\n",
    "#list(set(features[:,3]))\n",
    "\n",
    "# data1 = pd.read_csv('diabetes.csv')\n",
    "# # drop missing values\n",
    "# #data = data.dropna()\n",
    "# data1.isna().sum()\n",
    "\n",
    "# ft = data1.iloc[:, :-1].to_numpy()\n",
    "# tg = data1.iloc[:, -1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ft[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQ6fJv8ZhTGf"
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# onehotencoder = OneHotEncoder(sparse_output=False)\n",
    "# labels = features[:,2]\n",
    "# encoded = onehotencoder.fit_transform(labels.reshape(-1, 1))\n",
    "# #print(encoded[0])\n",
    "\n",
    "# ft1 = np.concatenate((features[:, :2], encoded, features[:, 3:]), axis=1)\n",
    "# print(\"new features \\n\", ft1[0])\n",
    "\n",
    "# print(\"Encoding shape: \", ft1[:,7].shape, list(set(ft1[:,7])))\n",
    "# # now we encode the marital status\n",
    "# encoded = onehotencoder.fit_transform(ft1[:,7].reshape(-1, 1))\n",
    "# print(\"Shape2: \", encoded.shape)\n",
    "# ft2 = np.concatenate((ft1[:, :7], encoded, ft1[:, 8:]), axis=1)\n",
    "# print(\"new features2 \\n\", ft2[0])\n",
    "\n",
    "# # removing the date column\n",
    "# ft2 = np.concatenate((ft2[:, :18], ft2[:, 19:]), axis=1)\n",
    "# print(\"final features2 \\n\", ft2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOS:\n",
    "- plot validation curve for KNN, SVM and ANN (for each dataset)\n",
    "- hyperparameter tuning:\n",
    "  - knn at least k,\n",
    "  - svm at least kernel, p etc\n",
    "  - ann: ? number of hidden layers, epoch count\n",
    "- performance vs training size for knn and svm\n",
    "- wall clock times (fit and predict times) for each algos and ds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Te7pbkHSzih"
   },
   "source": [
    "# RESULTS\n",
    "# KNN\n",
    "  # (.94,.96), (.95, 1.0)\n",
    "  # marketing:\n",
    "   # (.88,.85), (92, 84)\n",
    "\n",
    "# SvM\n",
    "  #(1,.9), (1,1 rbf),\n",
    "\n",
    "# ANN:\n",
    "  # 0.43,.35)\n",
    "  # marketing: (92,89), (1, 87), (.99, 86)\n",
    "============== RESULTS\n",
    "SCALE:\n",
    "\tKNN: .94,.83\n",
    "\tSVM: .91, .95\n",
    "\tANN, .00, .97\n",
    "UserKnowledge(257):  encoding ruins knn. svm poor with poly\n",
    "\tknn: 86,82\n",
    "\tsvm: 93,85\n",
    "\tann: 96,85\n",
    "\n",
    "MARKETING:\n",
    "\tKNN: (87, 81) on k4, (92,85) on k2\n",
    "\tSVM: (94, 86) on poly4, (92, 86) rbf\n",
    "\tANN: (1,86)\n",
    "\n",
    "\t\n",
    "IONOSPHERE:\n",
    "\tANN and SVM in 90s, knn in 80s.\n",
    "\tSVM linear best. Poly is poor"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
