{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "9YvNetPRPfzd"
   },
   "outputs": [],
   "source": [
    "# !pip install ucimlrepo\n",
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "mzX0_DV9PLT5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "5OtnwahasMGy"
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "BMPoZAOePeoz"
   },
   "outputs": [],
   "source": [
    "def get_data(name):\n",
    "    from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "    features, targets = np.array([]), np.array([])\n",
    "\n",
    "    # fetch dataset\n",
    "    if name == \"yeast\":\n",
    "        dataset = fetch_ucirepo(id=110)\n",
    "        features = dataset.data.features.to_numpy()\n",
    "        targets = dataset.data.targets.to_numpy()\n",
    "        targets = targets.ravel()\n",
    "    elif name == \"spambase\":\n",
    "        dataset = fetch_ucirepo(id=94)\n",
    "        features = dataset.data.features.to_numpy()\n",
    "        targets = dataset.data.targets.to_numpy()\n",
    "        targets = targets.ravel()\n",
    "    elif name == \"rice\":\n",
    "        dataset = fetch_ucirepo(id=545)\n",
    "        features = dataset.data.features.to_numpy()\n",
    "        targets = dataset.data.targets.to_numpy()\n",
    "        targets = targets.ravel()\n",
    "    elif name == \"churn\":\n",
    "        dataset = fetch_ucirepo(id=563)\n",
    "        features = dataset.data.features.to_numpy()\n",
    "        targets = dataset.data.targets.to_numpy()\n",
    "        targets = targets.ravel()\n",
    "    elif name == \"health_nutri\":\n",
    "        dataset = fetch_ucirepo(id=887)\n",
    "        features = dataset.data.features.to_numpy()\n",
    "        targets = dataset.data.targets.to_numpy()\n",
    "        targets = targets.ravel()\n",
    "    elif name == \"zoo\":    # Good small ds.\n",
    "      dataset = fetch_ucirepo(id=111)\n",
    "      features = dataset.data.features.to_numpy()\n",
    "      targets = dataset.data.targets.to_numpy()\n",
    "      targets = targets.ravel()\n",
    "    elif name == \"parkinsons\":    # 197x22 Good for all. Fit for comparisons\n",
    "      dataset = fetch_ucirepo(id=174)\n",
    "      features = dataset.data.features.to_numpy()\n",
    "      targets = dataset.data.targets.to_numpy()\n",
    "      targets = targets.ravel()\n",
    "    elif name == \"glass\":    # not that useful\n",
    "      dataset = fetch_ucirepo(id=42)\n",
    "      features = dataset.data.features.to_numpy()\n",
    "      targets = dataset.data.targets.to_numpy()\n",
    "      targets = targets.ravel()\n",
    "    elif name == \"thyroid\":  #useless\n",
    "        data = pd.read_csv('ann-train.data', header=None)\n",
    "\n",
    "        # Assuming the last column is the class label\n",
    "        features = data.iloc[:, :-1]\n",
    "        labels = data.iloc[:, -1]\n",
    "    elif name == \"marketing\":   # awesome for ANN\n",
    "        data = pd.read_csv('marketing_campaign.csv', sep=\"\\t\")\n",
    "        # drop missing values\n",
    "        data = data.dropna()\n",
    "\n",
    "        features = data.iloc[:, :-1].to_numpy()\n",
    "        targets = data.iloc[:, -1].to_numpy()\n",
    "        targets = targets.ravel()\n",
    "\n",
    "        # Encoding\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "        onehotencoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "        encoded = onehotencoder.fit_transform(features[:,2].reshape(-1, 1))\n",
    "\n",
    "        ft1 = np.concatenate((features[:, :2], encoded, features[:, 3:]), axis=1)\n",
    "        # now we encode the marital status\n",
    "        encoded = onehotencoder.fit_transform(ft1[:,7].reshape(-1, 1))\n",
    "        ft2 = np.concatenate((ft1[:, :7], encoded, ft1[:, 8:]), axis=1)\n",
    "\n",
    "        # removing the date column\n",
    "        ft2 = np.concatenate((ft2[:, :18], ft2[:, 19:]), axis=1)\n",
    "        features = ft2\n",
    "\n",
    "\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "lkxw4gftTYLx"
   },
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    onehotencoder = OneHotEncoder(sparse_output=False)\n",
    "    encoded_categories = onehotencoder.fit_transform(labels.reshape(-1, 1))\n",
    "\n",
    "    return encoded_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "r9FsZJ2Kjk-2"
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(model, features, labels):\n",
    "  features, labels = shuffle(features, labels, random_state=RANDOM_SEED)\n",
    "  from sklearn.model_selection import StratifiedKFold\n",
    "  cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "  # scale the features?\n",
    "  #scaler = StandardScaler()\n",
    "  #X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "  train_sizes, train_scores, test_scores = learning_curve(\n",
    "    model, features, labels, cv=5, scoring='accuracy', n_jobs=-1,\n",
    "    random_state=RANDOM_SEED, shuffle=True,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10)  # Varying the size of training data. also try (0.1, 1.0, 20)\n",
    "  )\n",
    "\n",
    "  # Calculate mean and standard deviation for train and test scores\n",
    "  train_mean = np.mean(train_scores, axis=1)\n",
    "  train_std = np.std(train_scores, axis=1)\n",
    "  test_mean = np.mean(test_scores, axis=1)\n",
    "  test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "  # Plot learning curve\n",
    "  plt.figure(figsize=(8, 6))\n",
    "  plt.plot(train_sizes, train_mean, label='Training score', color='blue')\n",
    "  plt.plot(train_sizes, test_mean, label='Cross-validation score', color='green')\n",
    "  # Plot the std deviation as a shaded region\n",
    "  plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color='blue', alpha=0.1)\n",
    "  plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color='green', alpha=0.1)\n",
    "\n",
    "  # Labeling the plot\n",
    "  plt.title('Learning Curve (Accuracy vs Training size)')\n",
    "  plt.xlabel('Training Set Size')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.legend(loc='best')\n",
    "  plt.grid()\n",
    "\n",
    "  # Show plot\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "fUPJl24MGYCM"
   },
   "outputs": [],
   "source": [
    "def plot_params_knn(p_values, weight_options, results):\n",
    "  # ---------------------- PLOTTING RESULTS ---------------------- #\n",
    "  # Plot the results to visualize performance of different hyperparameters\n",
    "  plt.figure(figsize=(12, 8))\n",
    "\n",
    "  # Create subplots for each distance metric (p=1, p=2)\n",
    "  for i, p_value in enumerate(p_values):\n",
    "      plt.subplot(1, 2, i + 1)\n",
    "      plt.title(f'KNN with p={p_value} (Manhattan' if p_value == 1 else 'Euclidean')\n",
    "\n",
    "      # Plot uniform and distance-based weights\n",
    "      for weights in weight_options:\n",
    "          subset = results[(results[:, 1] == p_value) & (results[:, 2] == weights)]\n",
    "          plt.plot(subset[:, 0], subset[:, 3], label=f'weights={weights}')\n",
    "\n",
    "      plt.xlabel('Number of Neighbors (n_neighbors)')\n",
    "      plt.ylabel('Cross-Validation Accuracy')\n",
    "      plt.legend()\n",
    "      plt.grid(True)\n",
    "\n",
    "  # Adjust layout and show the plot\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "D1RYwSMwDjS-"
   },
   "outputs": [],
   "source": [
    "def manually_tune_knn(X_train_scaled, ytrain):\n",
    "\n",
    "  # Define hyperparameter values to tune\n",
    "  n_neighbors_values = range(2, 31)  # Loop over neighbors from 1 to 30\n",
    "  p_values = [1, 2]  # 1: Manhattan, 2: Euclidean distance\n",
    "  weight_options = ['uniform', 'distance']  # Uniform or distance-based weighting\n",
    "\n",
    "  # Empty list to store the results\n",
    "  results = []\n",
    "\n",
    "  # Loop over all combinations of hyperparameters\n",
    "  for n_neighbors in n_neighbors_values:\n",
    "      for p in p_values:\n",
    "          for weights in weight_options:\n",
    "              # Create KNN model with current hyperparameters\n",
    "              knn = KNeighborsClassifier(n_neighbors=n_neighbors, p=p, weights=weights)\n",
    "\n",
    "              # Perform cross-validation (5-fold)\n",
    "              cv_scores = cross_val_score(knn, X_train_scaled, ytrain, cv=5, scoring='accuracy')\n",
    "\n",
    "              # Calculate the mean accuracy\n",
    "              mean_cv_score = np.mean(cv_scores)\n",
    "\n",
    "              # Store the result (n_neighbors, p, weights, mean_cv_score)\n",
    "              results.append((n_neighbors, p, weights, mean_cv_score))\n",
    "\n",
    "  # Convert results to a NumPy array for easier processing\n",
    "  results = np.array(results, dtype=object)\n",
    "\n",
    "  # Find the best hyperparameter combination (max accuracy)\n",
    "  best_index = np.argmax(results[:, 3].astype(float))\n",
    "  best_params = results[best_index]\n",
    "  best_n_neighbors = best_params[0]\n",
    "  best_p = best_params[1]\n",
    "  best_weights = best_params[2]\n",
    "  best_score = best_params[3]\n",
    "\n",
    "  # Print the best hyperparameters and their corresponding accuracy\n",
    "  print(f\"Best Hyperparameters: n_neighbors={best_n_neighbors}, p={best_p}, weights={best_weights}\")\n",
    "  print(f\"Best Cross-Validation Accuracy: {best_score:.4f}\")\n",
    "\n",
    "  plot_params_knn(p_values, weight_options, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "T0h-TpvOVgff"
   },
   "outputs": [],
   "source": [
    "def compare_k_values(xtrain, ytrain, data_name):\n",
    "    k_vals = [i for i in range(1,40)]\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    for k in k_vals:\n",
    "        #classifier = KNeighborsClassifier(n_neighbors = k, metric = 'minkowski', p = 2)\n",
    "        if data_name == \"marketing\":\n",
    "            classifier = KNeighborsClassifier(n_neighbors=k, p=2, algorithm=\"auto\", leaf_size=10, weights=\"distance\")\n",
    "        else:\n",
    "            classifier = KNeighborsClassifier(n_neighbors=k, p=1, algorithm=\"auto\", leaf_size=10, weights=\"distance\")\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_test = classifier.predict(X_test)\n",
    "        y_pred_train = classifier.predict(X_train)\n",
    "\n",
    "        train_accuracies.append(accuracy_score(y_train, y_pred_train))\n",
    "        test_accuracies.append(accuracy_score(y_test, y_pred_test))\n",
    "\n",
    "\n",
    "    # now let's plot it - Accuracy vs k\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(k_vals, train_accuracies, label='In-Sample Accuracy (Training Set)', marker='o', color='b')\n",
    "    plt.plot(k_vals, test_accuracies, label='Out-of-Sample Accuracy (Test Set)', marker='o', color='r')\n",
    "    plt.title('KNN Accuracy for Different Values of k')\n",
    "    plt.xlabel('Number of Neighbors (k)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(k_vals)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jf5HQpAkVgme"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "GV6yQ4yopAOs"
   },
   "outputs": [],
   "source": [
    "# plot svm params\n",
    "def plot_params_svm(gamma_values, results,degree_values):\n",
    "   # ---------------------- PLOTTING RESULTS ---------------------- #\n",
    "  # Separate results for plotting\n",
    "  linear_results = results[results[:, 1] == 'linear']\n",
    "  rbf_results = results[results[:, 1] == 'rbf']\n",
    "  poly_results = results[results[:, 1] == 'poly']\n",
    "\n",
    "  # Plot the results for linear kernel\n",
    "  plt.figure(figsize=(18, 6))\n",
    "\n",
    "  plt.subplot(1, 3, 1)\n",
    "  plt.title('SVM with Linear Kernel')\n",
    "  plt.plot(linear_results[:, 0], linear_results[:, 4], marker='o', label='Linear Kernel')\n",
    "  plt.xscale('log')\n",
    "  plt.xlabel('C (Regularization parameter)')\n",
    "  plt.ylabel('Cross-Validation Accuracy')\n",
    "  plt.grid(True)\n",
    "\n",
    "  # Plot the results for RBF kernel with different gamma values\n",
    "  plt.subplot(1, 3, 2)\n",
    "  plt.title('SVM with RBF Kernel')\n",
    "  for gamma in gamma_values:\n",
    "      subset = rbf_results[rbf_results[:, 2] == gamma]\n",
    "      plt.plot(subset[:, 0], subset[:, 4], marker='o', label=f'Gamma={gamma}')\n",
    "  plt.xscale('log')\n",
    "  plt.xlabel('C (Regularization parameter)')\n",
    "  plt.ylabel('Cross-Validation Accuracy')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "\n",
    "  # Plot the results for poly kernel with different degrees (p)\n",
    "  plt.subplot(1, 3, 3)\n",
    "  plt.title('SVM with Poly Kernel')\n",
    "  for degree in degree_values:\n",
    "      subset = poly_results[poly_results[:, 3] == degree]\n",
    "      plt.plot(subset[:, 0], subset[:, 4], marker='o', label=f'Degree={degree}')\n",
    "  plt.xscale('log')\n",
    "  plt.xlabel('C (Regularization parameter)')\n",
    "  plt.ylabel('Cross-Validation Accuracy')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "\n",
    "  # Adjust layout and show the plot\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "XmzugCMapCij"
   },
   "outputs": [],
   "source": [
    "def manually_tune_svm(X_train_scaled, ytrain):\n",
    "    # Define hyperparameter values to tune\n",
    "  C_values = np.logspace(-3, 2, 6)  # Different values of C (from 0.001 to 100)\n",
    "  gamma_values = ['scale', 'auto']  # Options for gamma (scale or auto)\n",
    "  kernel_options = ['linear', 'rbf', 'poly']  # We now include the poly kernel\n",
    "  degree_values = range(2, 7)  # Values of degree (p) for the poly kernel (from 2 to 6)\n",
    "\n",
    "  # Empty list to store the results\n",
    "  results = []\n",
    "\n",
    "  # Loop over all combinations of hyperparameters\n",
    "  for C in C_values:\n",
    "      for kernel in kernel_options:\n",
    "          if kernel == 'rbf':  # Only relevant for RBF kernel\n",
    "              for gamma in gamma_values:\n",
    "                  # Create SVM model with RBF kernel and current hyperparameters\n",
    "                  svm = SVC(C=C, kernel=kernel, gamma=gamma)\n",
    "\n",
    "                  # Perform cross-validation (5-fold)\n",
    "                  cv_scores = cross_val_score(svm, X_train_scaled, ytrain, cv=5, scoring='accuracy')\n",
    "\n",
    "                  # Calculate the mean accuracy\n",
    "                  mean_cv_score = np.mean(cv_scores)\n",
    "\n",
    "                  # Store the result (C, kernel, gamma, None, mean_cv_score)\n",
    "                  results.append((C, kernel, gamma, None, mean_cv_score))\n",
    "\n",
    "          elif kernel == 'poly':  # Only relevant for poly kernel\n",
    "              for degree in degree_values:\n",
    "                  # Create SVM model with poly kernel and current hyperparameters\n",
    "                  svm = SVC(C=C, kernel=kernel, degree=degree)\n",
    "\n",
    "                  # Perform cross-validation (5-fold)\n",
    "                  cv_scores = cross_val_score(svm, X_train_scaled, ytrain, cv=5, scoring='accuracy')\n",
    "\n",
    "                  # Calculate the mean accuracy\n",
    "                  mean_cv_score = np.mean(cv_scores)\n",
    "\n",
    "                  # Store the result (C, kernel, None, degree, mean_cv_score)\n",
    "                  results.append((C, kernel, None, degree, mean_cv_score))\n",
    "\n",
    "          else:  # Linear kernel\n",
    "              # Create SVM model with linear kernel\n",
    "              svm = SVC(C=C, kernel=kernel)\n",
    "\n",
    "              # Perform cross-validation (5-fold)\n",
    "              cv_scores = cross_val_score(svm, X_train_scaled, ytrain, cv=5, scoring='accuracy')\n",
    "\n",
    "              # Calculate the mean accuracy\n",
    "              mean_cv_score = np.mean(cv_scores)\n",
    "\n",
    "              # Store the result (C, kernel, None, None, mean_cv_score)\n",
    "              results.append((C, kernel, None, None, mean_cv_score))\n",
    "\n",
    "  # Convert results to a NumPy array for easier processing\n",
    "  results = np.array(results, dtype=object)\n",
    "\n",
    "  # Find the best hyperparameter combination (max accuracy)\n",
    "  best_index = np.argmax(results[:, 4].astype(float))\n",
    "  best_params = results[best_index]\n",
    "  best_C = best_params[0]\n",
    "  best_kernel = best_params[1]\n",
    "  best_gamma = best_params[2]  # Relevant for RBF kernel\n",
    "  best_degree = best_params[3]  # Relevant for Poly kernel\n",
    "  best_score = best_params[4]\n",
    "\n",
    "  # Print the best hyperparameters and their corresponding accuracy\n",
    "  print(f\"Best Hyperparameters: C={best_C}, kernel={best_kernel}, gamma={best_gamma}, degree={best_degree}\")\n",
    "  print(f\"Best Cross-Validation Accuracy: {best_score:.4f}\")\n",
    "\n",
    "  plot_params_svm(gamma_values,results,degree_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "hZUadbgRFer2"
   },
   "outputs": [],
   "source": [
    "def run_grid_search(xtrain, ytrain, model_type=\"knn\"):\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "    model = KNeighborsClassifier()\n",
    "    param_grid = {\n",
    "        'n_neighbors': range(1, 31),  # Test different k values\n",
    "        'weights': ['uniform', 'distance'],  # Test both uniform and distance weighting\n",
    "        'p': [1, 2],  # Test Manhattan and Euclidean distance\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],  # Test different search algorithms\n",
    "        'leaf_size': [10, 20, 30, 40, 50]  # Different leaf sizes for tree-based methods\n",
    "        }\n",
    "\n",
    "    if model_type == \"svm\":\n",
    "        model = SVC()\n",
    "        param_grid = {\n",
    "          'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "          'C': [0.1, 1, 10, 100],\n",
    "          'gamma': ['scale', 'auto'],\n",
    "          'degree': [2, 3, 4],  # Only relevant for 'poly' kernel\n",
    "          'coef0': [0, 0.1, 0.5, 1]  # Relevant for 'poly' and 'sigmoid' kernels\n",
    "       }\n",
    "    elif model_type == \"ann\":\n",
    "        model = MultilayerPerceptron()\n",
    "\n",
    "\n",
    "    # Set up the grid search with cross-validation\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1, shuffle=True)\n",
    "\n",
    "    # Fit the model\n",
    "    grid_search.fit(xtrain, ytrain)\n",
    "\n",
    "    return grid_search.best_params_, grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "wJeBGbEsWazU"
   },
   "outputs": [],
   "source": [
    "# Tuning with Optuna\n",
    "\n",
    "def run_optuna_ann(xtrain, ytrain, xtest, ytest):\n",
    "    import optuna\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    def create_model(trial):\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Dense(trial.suggest_int('units', 4, 128), activation='relu', input_shape=(xtrain.shape[1],)))\n",
    "        if ytrain.ndim == 1:\n",
    "            model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        else:\n",
    "            model.add(layers.Dense(ytrain.shape[1], activation='softmax'))\n",
    "\n",
    "        optimizer = trial.suggest_categorical('optimizer', ['adam', 'rmsprop', 'sgd'])\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy' if ytrain.ndim ==1 else 'categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def objective(trial):\n",
    "        model = create_model(trial)\n",
    "        model.fit(xtrain, ytrain, epochs=trial.suggest_int('epochs', 10, 50), batch_size=trial.suggest_int('batch_size', 16, 64), verbose=0)\n",
    "        score = model.evaluate(xtest, ytest, verbose=0)\n",
    "        return score[1]\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=50)\n",
    "\n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print('  Value: {}'.format(trial.value))\n",
    "    print('  Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print('    {}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "mKXizGAWBXMa"
   },
   "outputs": [],
   "source": [
    "def plot_fit_and_predict_times(X, y, models=None):\n",
    "    # Split into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Standardize the dataset (important for SVM and ANN)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Define the models to benchmark\n",
    "    if models is None:\n",
    "      models = {\n",
    "          'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "          'SVM': SVC(kernel='rbf', C=1, gamma='scale'),\n",
    "          'ANN': MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000)\n",
    "      }\n",
    "\n",
    "    # Empty dictionaries to store timing results\n",
    "    fit_times = {}\n",
    "    predict_times = {}\n",
    "\n",
    "    # Loop over models to measure fit and predict times\n",
    "    for model_name, model in models.items():\n",
    "        # Measure the time taken to fit the model\n",
    "        start_fit = time.time()\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        end_fit = time.time()\n",
    "\n",
    "        # Measure the time taken to predict using the model\n",
    "        start_predict = time.time()\n",
    "        model.predict(X_test_scaled)\n",
    "        end_predict = time.time()\n",
    "\n",
    "        # Calculate and store the elapsed times\n",
    "        fit_times[model_name] = end_fit - start_fit\n",
    "        predict_times[model_name] = end_predict - start_predict\n",
    "\n",
    "    # Print out the results\n",
    "    print(\"Fit Times (in seconds):\", fit_times)\n",
    "    print(\"Predict Times (in seconds):\", predict_times)\n",
    "\n",
    "    # ---------------------- PLOTTING RESULTS ---------------------- #\n",
    "    # Set up the plot\n",
    "    model_names = list(models.keys())\n",
    "    fit_values = list(fit_times.values())\n",
    "    predict_values = list(predict_times.values())\n",
    "\n",
    "    bar_width = 0.35  # Width of the bars\n",
    "    index = np.arange(len(model_names))  # X locations for the models\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Bar chart for fit times\n",
    "    plt.bar(index, fit_values, bar_width, label='Fit Time')\n",
    "\n",
    "    # Bar chart for predict times (stacked beside fit times)\n",
    "    plt.bar(index + bar_width, predict_values, bar_width, label='Predict Time')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.title('Fit and Predict Times for KNN, SVM, and ANN')\n",
    "    plt.xticks(index + bar_width / 2, model_names)\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "yZSpp9K5BVus"
   },
   "outputs": [],
   "source": [
    "def run_ann_validation_plot(xtrain, ytrain, xtest, ytest):\n",
    "\n",
    "    def create_model(num_hidden_layers, units_per_layer):\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Dense(units_per_layer, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            model.add(layers.Dense(units_per_layer, activation='relu'))\n",
    "\n",
    "        if ytrain.ndim == 1:\n",
    "            model.add(layers.Dense(1, activation='sigmoid'))\n",
    "            model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        else:\n",
    "            model.add(layers.Dense(ytrain.shape[1], activation='softmax'))\n",
    "            model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    num_hidden_layers_list = [1, 2, 3]\n",
    "    epochs_list = [10, 30, 50, 70, 90, 110, 150]\n",
    "    results = {}\n",
    "\n",
    "    for num_hidden_layers in num_hidden_layers_list:\n",
    "        for epochs in epochs_list:\n",
    "            model = create_model(num_hidden_layers, 64)\n",
    "            history = model.fit(xtrain, ytrain, epochs=epochs, batch_size=32, validation_split=0.2, verbose=0)\n",
    "            train_acc = history.history['accuracy'][-1]\n",
    "            test_loss, test_acc = model.evaluate(xtest, ytest, verbose=0)\n",
    "            #results[(num_hidden_layers, epochs)] = test_acc\n",
    "            results[(num_hidden_layers, epochs)] = (train_acc, test_acc)\n",
    "\n",
    "    # # Plot the results\n",
    "    # fig, ax = plt.subplots()\n",
    "    # for num_hidden_layers in num_hidden_layers_list:\n",
    "    #     accuracies = [results[(num_hidden_layers, epochs)] for epochs in epochs_list]\n",
    "    #     ax.plot(epochs_list, accuracies, label=f'{num_hidden_layers} hidden layers')\n",
    "\n",
    "    # ax.set_xlabel('Epochs')\n",
    "    # ax.set_ylabel('Accuracy')\n",
    "    # ax.set_title('Effect of Number of Hidden Layers and Epochs on Model Performance')\n",
    "    # ax.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    # Plot the results\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    for num_hidden_layers in num_hidden_layers_list:\n",
    "        train_accuracies = [results[(num_hidden_layers, epochs)][0] for epochs in epochs_list]\n",
    "        test_accuracies = [results[(num_hidden_layers, epochs)][1] for epochs in epochs_list]\n",
    "        ax[0].plot(epochs_list, train_accuracies, label=f'{num_hidden_layers} hidden layers')\n",
    "        ax[1].plot(epochs_list, test_accuracies, label=f'{num_hidden_layers} hidden layers')\n",
    "\n",
    "    ax[0].set_xlabel('Epochs')\n",
    "    ax[0].set_ylabel('Training Accuracy')\n",
    "    ax[0].set_title('Training Accuracy vs. Epochs')\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].set_xlabel('Epochs')\n",
    "    ax[1].set_ylabel('Test Accuracy')\n",
    "    ax[1].set_title('Test Accuracy vs. Epochs')\n",
    "    ax[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giG7kAxjfb8L"
   },
   "source": [
    "# RUN ALL\n",
    "- get dataset 1\n",
    "- get dataset 2\n",
    "- run gridsearch or manual tune for ds1\n",
    "- plot validation plots (hyperparameters tuning) ds1\n",
    "- learning curve KNN, SVM, ANN1, ANN2 for ds1\n",
    "\n",
    "- Repeat for ds2\n",
    "- do decision tree and implement boosting for ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "wAjbeidbkcfS"
   },
   "outputs": [],
   "source": [
    "# RUN DS 1 -> zoo for now\n",
    "\n",
    "# Get the first dataset\n",
    "  # scale features and labels.\n",
    "X, y = get_data(\"zoo\")\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "y_encoded = encode_onehot(y)  # oneHot encode zoo since it's multicalss\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# now scale the features.\n",
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.transform(X_test)\n",
    "\n",
    "# Run KNN manual parameter tuning\n",
    "# manually_tune_knn(X, y)   # this should return the best model based on the tuning\n",
    "# knn = KNeighborsClassifier(n_neighbors=8, metric='minkowski', p=2)  # replace with the optimized params\n",
    "# plot_learning_curve(knn, X, y)\n",
    "\n",
    "# Run svm tuning and learning curve\n",
    "# manually_tune_svm(X, y)\n",
    "# svm = SVC(kernel=\"poly\", degree=3, random_state=0)\n",
    "# plot_learning_curve(svm, X, y)\n",
    "\n",
    "# Run Gridsearch (Optuna)  for ANN and plot LC\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=0)\n",
    "# run_optuna_ann(X_train, y_train, X_test, y_test)\n",
    "# run_ann_validation_plot(X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "# knn = KNeighborsClassifier(n_neighbors=8, metric='minkowski', p=2)\n",
    "\n",
    "# plot_fit_and_predict_times(X, y)\n",
    "#bestParams, best_score = run_grid_search(X, y, model_type=\"svm\")\n",
    "#print(f\"Grid search results: Best params: {bestParams} \\nBest score: {best_score}\")\n",
    "\n",
    "# update the code for fit and predict times\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l0_8dEYPPtwz"
   },
   "outputs": [],
   "source": [
    "def run_knn(data_name, encode_type=None):\n",
    "    X, y = get_data(data_name)\n",
    "    test_size = 0.20\n",
    "\n",
    "    if encode_type == \"onehot\":\n",
    "        y = encode_onehot(y)\n",
    "    elif encode_type == \"label\":\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "\n",
    "    # splitting into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=0)\n",
    "\n",
    "    # feature scaling\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "    # run grid_search for hyperparameter tuning.\n",
    "    # print(\"Running grid search\")\n",
    "    # best_params, best_scores = run_grid_search(X_train, y_train)\n",
    "    # print(f\"Grid search done. Best params: {best_params} \\nBest score: {best_scores}\")\n",
    "\n",
    "\n",
    "    classifier = KNeighborsClassifier(n_neighbors=8, metric='minkowski', p=2)\n",
    "    #classifier = KNeighborsClassifier(n_neighbors=5, weights=\"distance\")\n",
    "    # classifier = KNeighborsClassifier(n_neighbors=30, p=2, algorithm=\"auto\", leaf_size=10, weights=\"distance\") #{'algorithm': 'auto', 'leaf_size': 10, 'n_neighbors': 30, 'p': 1, 'weights': 'distance'}\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # predicting on training and test sets\n",
    "    y_pred_train = classifier.predict(X_train)\n",
    "    y_pred_test = classifier.predict(X_test)\n",
    "\n",
    "    print(\"pridiction done.\")\n",
    "    # cm_train = confusion_matrix(y_train, y_pred_train)\n",
    "    print(f\"Insample prediction for {data_name}= {accuracy_score(y_train, y_pred_train)}\")\n",
    "    print(f\"Out-sample prediction for {data_name}= {accuracy_score(y_test, y_pred_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3t020MpUTu42"
   },
   "outputs": [],
   "source": [
    "def run_ann(data_name, encode_type=None):\n",
    "    import random\n",
    "    seed_value = 0\n",
    "    # 1. Set seed for Python's random module\n",
    "    random.seed(seed_value)\n",
    "    # 2. Set seed for NumPy\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "    # 3. Set seed for TensorFlow\n",
    "    tf.random.set_seed(seed_value)\n",
    "\n",
    "\n",
    "    X, y = get_data(data_name)\n",
    "    test_size = 0.30\n",
    "\n",
    "    if encode_type == \"onehot\":\n",
    "        y = encode_onehot(y)\n",
    "    elif encode_type == \"label\":\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "\n",
    "    # splitting into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=0)\n",
    "\n",
    "    # feature scaling\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "    # run grid_search for hyperparameter tuning.\n",
    "\n",
    "    # create the ANN model\n",
    "    # initialize the ann\n",
    "    ann = tf.keras.models.Sequential()\n",
    "    ann.add(tf.keras.layers.Dense(units=X.shape[1], activation='relu')) # input and first hidden layer\n",
    "    ann.add(tf.keras.layers.Dense(units=64, activation='relu'))  # 2nd hidden layer\n",
    "    ann.add(tf.keras.layers.Dense(units=64, activation='relu'))  # 3rd hidden layer\n",
    "\n",
    "    if y.ndim == 1:\n",
    "        ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "        ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    else:\n",
    "        ann.add(tf.keras.layers.Dense(units=y.shape[1], activation='softmax'))\n",
    "        ann.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    ann.fit(X_train, y_train, batch_size=32, epochs=100, shuffle=False)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_train = ann.predict(X_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss_train, accuracy_train = ann.evaluate(X_train, y_train)\n",
    "    loss_test, accuracy_test = ann.evaluate(X_test, y_test)\n",
    "    print(f\"Train Accuracy: {accuracy_train:.2f}\")\n",
    "    print(f\"Test Accuracy: {accuracy_test:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9S3EE-KSvOz"
   },
   "outputs": [],
   "source": [
    "def run_svm(data_name, encode_type=None):\n",
    "    X, y = get_data(data_name)\n",
    "    test_size = 0.20\n",
    "\n",
    "    if encode_type == \"onehot\":\n",
    "        y = encode_onehot(y)\n",
    "    elif encode_type == \"label\":\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "\n",
    "    # splitting into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=0)\n",
    "\n",
    "    # feature scaling\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "    # run grid_search for hyperparameter tuning.\n",
    "    # print(\"Running grid search\")\n",
    "    # best_params, best_scores = run_grid_search(X_train, y_train)\n",
    "    # print(f\"Grid search done. Best params: {best_params} \\nBest score: {best_scores}\")\n",
    "\n",
    "    print(\"p3\")\n",
    "    # training on train set\n",
    "    classifier = SVC(kernel=\"rbf\", degree=2, random_state=0)\n",
    "    #classifier = SVC(kernel=\"poly\", degree=4, random_state=0)\n",
    "    #classifier = SVC(kernel=\"rbf\", degree=3, random_state=0)\n",
    "    print(\"p1\")\n",
    "    classifier.fit(X_train, y_train)\n",
    "    print(\"p2\")\n",
    "    # predicting on training and test sets\n",
    "    y_pred_train = classifier.predict(X_train)\n",
    "    y_pred_test = classifier.predict(X_test)\n",
    "\n",
    "\n",
    "    print(\"pridiction done.\")\n",
    "    #cm_train = confusion_matrix(y_train, y_pred_train)\n",
    "    print(f\"Insample prediction for {data_name}= {accuracy_score(y_train, y_pred_train)}\")\n",
    "    print(f\"Out-sample prediction for {data_name}= {accuracy_score(y_test, y_pred_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvGXSDWZRk_G"
   },
   "outputs": [],
   "source": [
    "# run_ann(\"zoo\")   # perfect against ann\n",
    "#run_knn(\"marketing\")\n",
    "# run_ann(\"marketing\")  # best for ANN\n",
    "#run_svm(\"parkinsons\")\n",
    "#run_knn(\"glass\")    # don't encode y here\n",
    "#run_ann(\"glass\", encode_type=\"onehot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "id": "q4Br9qL-f3v5",
    "outputId": "4a1cb2ef-eebc-44eb-c41e-fe6693c10434"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-21 16:49:39,353] A new study created in memory with name: no-name-bb0051c3-4a9b-4293-8595-7c5b8a7df3db\n",
      "C:\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "[I 2024-09-21 16:49:45,074] Trial 0 finished with value: 0.869369387626648 and parameters: {'units': 116, 'optimizer': 'adam', 'epochs': 18, 'batch_size': 51}. Best is trial 0 with value: 0.869369387626648.\n",
      "[I 2024-09-21 16:49:49,620] Trial 1 finished with value: 0.8851351141929626 and parameters: {'units': 65, 'optimizer': 'adam', 'epochs': 16, 'batch_size': 58}. Best is trial 1 with value: 0.8851351141929626.\n",
      "[I 2024-09-21 16:50:00,983] Trial 2 finished with value: 0.8626126050949097 and parameters: {'units': 85, 'optimizer': 'adam', 'epochs': 36, 'batch_size': 21}. Best is trial 1 with value: 0.8851351141929626.\n",
      "[I 2024-09-21 16:50:08,530] Trial 3 finished with value: 0.8828828930854797 and parameters: {'units': 69, 'optimizer': 'sgd', 'epochs': 42, 'batch_size': 50}. Best is trial 1 with value: 0.8851351141929626.\n",
      "[I 2024-09-21 16:50:13,387] Trial 4 finished with value: 0.8851351141929626 and parameters: {'units': 55, 'optimizer': 'sgd', 'epochs': 24, 'batch_size': 45}. Best is trial 1 with value: 0.8851351141929626.\n",
      "[I 2024-09-21 16:50:19,050] Trial 5 finished with value: 0.869369387626648 and parameters: {'units': 16, 'optimizer': 'sgd', 'epochs': 29, 'batch_size': 44}. Best is trial 1 with value: 0.8851351141929626.\n",
      "[I 2024-09-21 16:50:24,186] Trial 6 finished with value: 0.8828828930854797 and parameters: {'units': 35, 'optimizer': 'adam', 'epochs': 21, 'batch_size': 57}. Best is trial 1 with value: 0.8851351141929626.\n",
      "[I 2024-09-21 16:50:29,752] Trial 7 finished with value: 0.8648648858070374 and parameters: {'units': 43, 'optimizer': 'sgd', 'epochs': 27, 'batch_size': 42}. Best is trial 1 with value: 0.8851351141929626.\n",
      "[I 2024-09-21 16:50:39,072] Trial 8 finished with value: 0.8626126050949097 and parameters: {'units': 121, 'optimizer': 'adam', 'epochs': 38, 'batch_size': 31}. Best is trial 1 with value: 0.8851351141929626.\n",
      "[I 2024-09-21 16:50:43,175] Trial 9 finished with value: 0.8738738894462585 and parameters: {'units': 19, 'optimizer': 'sgd', 'epochs': 10, 'batch_size': 16}. Best is trial 1 with value: 0.8851351141929626.\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL FOR THE MARKETING DATASET\n",
    "\n",
    "X, y = get_data(\"marketing\")\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# now scale the features.\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Run KNN manual parameter tuning\n",
    "# manually_tune_knn(X, y)   # this should return the best model based on the tuning\n",
    "# # best params: k=15, p=2, weights = distance. Accuracy = 0.8718\n",
    "# compare_k_values(X_train, y_train, \"marketing\")\n",
    "# knn = KNeighborsClassifier(n_neighbors=15, metric='minkowski', p=2)  # replace with the optimized params\n",
    "# plot_learning_curve(knn, X, y)\n",
    "\n",
    "# Run svm tuning and learning curve\n",
    "#manually_tune_svm(X, y)\n",
    "#svm = SVC(kernel=\"linear\", C=10, random_state=0)\n",
    "#plot_learning_curve(svm, X, y)\n",
    "\n",
    "# Run Gridsearch (Optuna)  for ANN and plot LC\n",
    "y_encoded = encode_onehot(y)  # oneHot encode zoo since it's multicalss\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=0)\n",
    "run_optuna_ann(X_train, y_train, X_test, y_test)\n",
    "# run_ann_validation_plot(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
